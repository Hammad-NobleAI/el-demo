{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hammadrehman/.pyenv/versions/3.8.13/envs/el-demo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets import Dataset\n",
    "# from datasets import load_metric\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification, BertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 18.7kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 208k/208k [00:00<00:00, 3.84MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 426k/426k [00:00<00:00, 5.69MB/s]\n",
      "Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 471kB/s]\n"
     ]
    }
   ],
   "source": [
    "task = \"ner\" \n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "batch_size = 16\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = {}\n",
    "entities = {}\n",
    "tags = set()\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"chemdner_corpus/training.abstracts.txt\", \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in csvreader:\n",
    "        abstracts[row[0]] = row[2]\n",
    "\n",
    "with open(\"chemdner_corpus/training.annotations.txt\", \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in csvreader:\n",
    "        if row[0] in entities:\n",
    "            entities[row[0]][row[4]] = row[5]\n",
    "            tags.add(row[5])\n",
    "        else:\n",
    "            entities[row[0]] = {row[4]:row[5]}\n",
    "            tags.add(row[5])\n",
    "\n",
    "with open(\"chemdner_corpus/development.abstracts.txt\", \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in csvreader:\n",
    "        if row[0] in abstracts.keys():\n",
    "            print(\"IN IN IN\")\n",
    "        abstracts[row[0]] = row[2]\n",
    "\n",
    "with open(\"chemdner_corpus/development.annotations.txt\", \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in csvreader:\n",
    "        if row[0] in entities:\n",
    "            entities[row[0]][row[4]] = row[5]\n",
    "            tags.add(row[5])\n",
    "        else:\n",
    "            entities[row[0]] = {row[4]:row[5]}\n",
    "            tags.add(row[5])\n",
    "\n",
    "abstracts_test = {}\n",
    "entities_test = {}\n",
    "\n",
    "with open(\"chemdner_corpus/evaluation.abstracts.txt\", \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in csvreader:\n",
    "        abstracts_test[row[0]] = row[2]\n",
    "\n",
    "with open(\"chemdner_corpus/evaluation.annotations.txt\", \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in csvreader:\n",
    "        if row[0] in entities_test:\n",
    "            entities_test[row[0]][row[4]] = row[5]\n",
    "        else:\n",
    "            entities_test[row[0]] = {row[4]:row[5]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NO CLASS',\n",
       " 1: 'ABBREVIATION',\n",
       " 2: 'MULTIPLE',\n",
       " 3: 'IDENTIFIER',\n",
       " 4: 'FORMULA',\n",
       " 5: 'FAMILY',\n",
       " 6: 'TRIVIAL',\n",
       " 7: 'SYSTEMATIC',\n",
       " 8: 'O'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {k:v for v, k in enumerate(tags)}\n",
    "label2id['O'] = 8\n",
    "\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We implemented a two-step approach to detect p...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aflatoxicosis is a cause of economic losses in...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The aim of this study was to investigate the e...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nuclear factor-κB ( NF-κB ) is a transcription...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chromium is widely used in the leather industr...</td>\n",
       "      <td>SYSTEMATIC,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  We implemented a two-step approach to detect p...   \n",
       "1  Aflatoxicosis is a cause of economic losses in...   \n",
       "2  The aim of this study was to investigate the e...   \n",
       "3  Nuclear factor-κB ( NF-κB ) is a transcription...   \n",
       "4  Chromium is widely used in the leather industr...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
       "1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
       "2  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
       "3  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
       "4  SYSTEMATIC,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In this work , bioconjugation techniques are d...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,ABBREVIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract 1 . Organic anion transporting polype...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mucin 1 ( MUC1 ) is a heterodimeric protein fo...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The potential-controlled incorporation of DOPC...</td>\n",
       "      <td>O,O,O,O,ABBREVIATION,O,O,O,O,O,O,O,O,O,SYSTEMA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grape seed phenolic extract ( GSE ) is predict...</td>\n",
       "      <td>O,O,FAMILY,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  In this work , bioconjugation techniques are d...   \n",
       "1  Abstract 1 . Organic anion transporting polype...   \n",
       "2  Mucin 1 ( MUC1 ) is a heterodimeric protein fo...   \n",
       "3  The potential-controlled incorporation of DOPC...   \n",
       "4  Grape seed phenolic extract ( GSE ) is predict...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,ABBREVIA...  \n",
       "1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
       "2  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
       "3  O,O,O,O,ABBREVIATION,O,O,O,O,O,O,O,O,O,SYSTEMA...  \n",
       "4  O,O,FAMILY,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "def get_tokens(abstract):\n",
    "    text = abstracts[abstract]\n",
    "    if abstract not in entities.keys():\n",
    "        return\n",
    "    tokenized = word_tokenize(text)\n",
    "    entities_temp = []\n",
    "    for token in tokenized:\n",
    "        if token in entities[abstract].keys():\n",
    "            entities_temp.append(entities[abstract][token])\n",
    "        else:\n",
    "            entities_temp.append('O')\n",
    "    return pd.DataFrame({'tokens':[tokenized], 'ner_tags': [entities_temp]})\n",
    "\n",
    "def get_tokens_test(abstract):\n",
    "    text = abstracts_test[abstract]\n",
    "    if abstract not in entities_test.keys():\n",
    "        return\n",
    "    tokenized = word_tokenize(text)\n",
    "    entities_temp = []\n",
    "    for token in tokenized:\n",
    "        if token in entities_test[abstract].keys():\n",
    "            entities_temp.append(entities_test[abstract][token])\n",
    "        else:\n",
    "            entities_temp.append('O')\n",
    "    return pd.DataFrame({'tokens':[tokenized], 'ner_tags': [entities_temp]})\n",
    "\n",
    "def get_all_tokens_and_ner_tags():\n",
    "    return pd.concat([get_tokens(abstract) for abstract in abstracts.keys()]).reset_index().drop('index', axis=1), pd.concat([get_tokens_test(abstract) for abstract in abstracts_test.keys()]).reset_index().drop('index', axis=1)\n",
    "\n",
    "all_data, test_data = get_all_tokens_and_ner_tags()\n",
    "all_data[\"sentence\"] = all_data[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "all_data[\"word_labels\"] = all_data[\"ner_tags\"].apply(lambda x: \",\".join(x))\n",
    "all_data = all_data[[\"sentence\", \"word_labels\"]]\n",
    "test_data[\"sentence\"] = test_data[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "test_data[\"word_labels\"] = test_data[\"ner_tags\"].apply(lambda x: \",\".join(x))\n",
    "test_data = test_data[[\"sentence\", \"word_labels\"]]\n",
    "\n",
    "display(all_data.head())\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda\")\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]  \n",
    "        word_labels = self.data.word_labels[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "        \n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long, device=cuda),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long, device=cuda),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long, device=cuda)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "\n",
    "train_dataset = all_data\n",
    "test_dataset = test_data\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 416M/416M [00:10<00:00, 41.1MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', \n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "\n",
    "model.to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 2.1830191612243652\n",
      "Training loss per 100 training steps: 0.37859700966884596\n",
      "Training loss per 100 training steps: 0.2735012875712333\n",
      "Training loss per 100 training steps: 0.22399659811262276\n",
      "Training loss per 100 training steps: 0.19492416736899765\n",
      "Training loss per 100 training steps: 0.17800685683796982\n",
      "Training loss per 100 training steps: 0.16333800518324293\n",
      "Training loss per 100 training steps: 0.15203170280388095\n",
      "Training loss per 100 training steps: 0.1428989600250672\n",
      "Training loss per 100 training steps: 0.13461627772534793\n",
      "Training loss per 100 training steps: 0.12766187958285347\n",
      "Training loss per 100 training steps: 0.12219696822445113\n",
      "Training loss per 100 training steps: 0.11724404280665614\n",
      "Training loss per 100 training steps: 0.1133395359306657\n",
      "Training loss per 100 training steps: 0.10958981677639876\n",
      "Training loss epoch: 0.10757131044302586\n",
      "Training accuracy epoch: 0.952765001052807\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.04464920237660408\n",
      "Training loss per 100 training steps: 0.04597364573066335\n",
      "Training loss per 100 training steps: 0.04650291497239945\n",
      "Training loss per 100 training steps: 0.045102141907382945\n",
      "Training loss per 100 training steps: 0.0451648535806119\n",
      "Training loss per 100 training steps: 0.046091907922580035\n",
      "Training loss per 100 training steps: 0.04586288953504483\n",
      "Training loss per 100 training steps: 0.04596059851181666\n",
      "Training loss per 100 training steps: 0.04590971436140508\n",
      "Training loss per 100 training steps: 0.04516491848785745\n",
      "Training loss per 100 training steps: 0.04558421041480136\n",
      "Training loss per 100 training steps: 0.04531145926836039\n",
      "Training loss per 100 training steps: 0.04500513512563258\n",
      "Training loss per 100 training steps: 0.044975231409854585\n",
      "Training loss per 100 training steps: 0.04499740705946374\n",
      "Training loss epoch: 0.044934554938143786\n",
      "Training accuracy epoch: 0.9775923025541643\n",
      "Training epoch: 3\n",
      "Training loss per 100 training steps: 0.03609248623251915\n",
      "Training loss per 100 training steps: 0.02936384570323816\n",
      "Training loss per 100 training steps: 0.029827274835033724\n",
      "Training loss per 100 training steps: 0.03004224296037318\n",
      "Training loss per 100 training steps: 0.030645024589961975\n",
      "Training loss per 100 training steps: 0.030567987203554338\n",
      "Training loss per 100 training steps: 0.031178950213268553\n",
      "Training loss per 100 training steps: 0.03155624218155217\n",
      "Training loss per 100 training steps: 0.030921864028646263\n",
      "Training loss per 100 training steps: 0.031253004034262226\n",
      "Training loss per 100 training steps: 0.031091949586897483\n",
      "Training loss per 100 training steps: 0.03100734755488199\n",
      "Training loss per 100 training steps: 0.031001799552624644\n",
      "Training loss per 100 training steps: 0.031051049969727452\n",
      "Training loss per 100 training steps: 0.031067086399488315\n",
      "Training loss epoch: 0.031018357734825298\n",
      "Training accuracy epoch: 0.9843604529404725\n",
      "Training epoch: 4\n",
      "Training loss per 100 training steps: 0.008590164594352245\n",
      "Training loss per 100 training steps: 0.023009890918666037\n",
      "Training loss per 100 training steps: 0.022242049168823146\n",
      "Training loss per 100 training steps: 0.021983696096553543\n",
      "Training loss per 100 training steps: 0.021675569596822302\n",
      "Training loss per 100 training steps: 0.021637647357065706\n",
      "Training loss per 100 training steps: 0.02167670447048119\n",
      "Training loss per 100 training steps: 0.022267740656430442\n",
      "Training loss per 100 training steps: 0.022147871963458878\n",
      "Training loss per 100 training steps: 0.02236347167430183\n",
      "Training loss per 100 training steps: 0.022805829870525738\n",
      "Training loss per 100 training steps: 0.022889128102712437\n",
      "Training loss per 100 training steps: 0.022708137000053672\n",
      "Training loss per 100 training steps: 0.022875531033555878\n",
      "Training loss per 100 training steps: 0.022623692306171075\n",
      "Training loss epoch: 0.022758615541313924\n",
      "Training accuracy epoch: 0.988648981658993\n",
      "Training epoch: 5\n",
      "Training loss per 100 training steps: 0.027584973722696304\n",
      "Training loss per 100 training steps: 0.017482267164205673\n",
      "Training loss per 100 training steps: 0.01826678639737106\n",
      "Training loss per 100 training steps: 0.01756338156329273\n",
      "Training loss per 100 training steps: 0.017081311858822928\n",
      "Training loss per 100 training steps: 0.016848730052388673\n",
      "Training loss per 100 training steps: 0.01711217268111886\n",
      "Training loss per 100 training steps: 0.017432203073703554\n",
      "Training loss per 100 training steps: 0.01743701098234766\n",
      "Training loss per 100 training steps: 0.017889701047114775\n",
      "Training loss per 100 training steps: 0.01808864094979166\n",
      "Training loss per 100 training steps: 0.017948960158523496\n",
      "Training loss per 100 training steps: 0.01791611908105079\n",
      "Training loss per 100 training steps: 0.017796223828455235\n",
      "Training loss per 100 training steps: 0.01772637762425788\n",
      "Training loss epoch: 0.017701931528690793\n",
      "Training accuracy epoch: 0.9911054213721865\n",
      "Training epoch: 6\n",
      "Training loss per 100 training steps: 0.00557525921612978\n",
      "Training loss per 100 training steps: 0.013877057882157238\n",
      "Training loss per 100 training steps: 0.013274060759319233\n",
      "Training loss per 100 training steps: 0.012628605791160286\n",
      "Training loss per 100 training steps: 0.013072873826575305\n",
      "Training loss per 100 training steps: 0.012940661944174757\n",
      "Training loss per 100 training steps: 0.013195761809700453\n",
      "Training loss per 100 training steps: 0.013687430308187132\n",
      "Training loss per 100 training steps: 0.01355180906394343\n",
      "Training loss per 100 training steps: 0.013624285752047783\n",
      "Training loss per 100 training steps: 0.013989153067162911\n",
      "Training loss per 100 training steps: 0.0139829771157105\n",
      "Training loss per 100 training steps: 0.013867844842255292\n",
      "Training loss per 100 training steps: 0.013913122220862807\n",
      "Training loss per 100 training steps: 0.013909000436169461\n",
      "Training loss epoch: 0.014112326863951339\n",
      "Training accuracy epoch: 0.9928623405699596\n",
      "Training epoch: 7\n",
      "Training loss per 100 training steps: 0.01863083988428116\n",
      "Training loss per 100 training steps: 0.012146402216092268\n",
      "Training loss per 100 training steps: 0.01147627908434135\n",
      "Training loss per 100 training steps: 0.011552238061040195\n",
      "Training loss per 100 training steps: 0.011327611117824352\n",
      "Training loss per 100 training steps: 0.011166412791381534\n",
      "Training loss per 100 training steps: 0.011423956013543991\n",
      "Training loss per 100 training steps: 0.011879153417292338\n",
      "Training loss per 100 training steps: 0.011599826062751874\n",
      "Training loss per 100 training steps: 0.011621920406178429\n",
      "Training loss per 100 training steps: 0.011502167023075806\n",
      "Training loss per 100 training steps: 0.011535806060826501\n",
      "Training loss per 100 training steps: 0.011600206199666344\n",
      "Training loss per 100 training steps: 0.011496121369776614\n",
      "Training loss per 100 training steps: 0.011580871207089739\n",
      "Training loss epoch: 0.01164716476032881\n",
      "Training accuracy epoch: 0.9940780471247205\n",
      "Training epoch: 8\n",
      "Training loss per 100 training steps: 0.005044938530772924\n",
      "Training loss per 100 training steps: 0.0097585908626552\n",
      "Training loss per 100 training steps: 0.00861357783102917\n",
      "Training loss per 100 training steps: 0.00902102372284779\n",
      "Training loss per 100 training steps: 0.009607361789603783\n",
      "Training loss per 100 training steps: 0.00969293933384889\n",
      "Training loss per 100 training steps: 0.009410912521922854\n",
      "Training loss per 100 training steps: 0.009509761475150164\n",
      "Training loss per 100 training steps: 0.009481691523795464\n",
      "Training loss per 100 training steps: 0.009486811127853203\n",
      "Training loss per 100 training steps: 0.009686184660132675\n",
      "Training loss per 100 training steps: 0.009598738114707144\n",
      "Training loss per 100 training steps: 0.009500459502466146\n",
      "Training loss per 100 training steps: 0.009440954127645142\n",
      "Training loss per 100 training steps: 0.00952120075623506\n",
      "Training loss epoch: 0.009511998971187754\n",
      "Training accuracy epoch: 0.9951863736048077\n",
      "Training epoch: 9\n",
      "Training loss per 100 training steps: 0.011249932460486889\n",
      "Training loss per 100 training steps: 0.0062612006811040115\n",
      "Training loss per 100 training steps: 0.007682318134877405\n",
      "Training loss per 100 training steps: 0.007550375636763124\n",
      "Training loss per 100 training steps: 0.008238541190995848\n",
      "Training loss per 100 training steps: 0.008284988837253777\n",
      "Training loss per 100 training steps: 0.007843620874501567\n",
      "Training loss per 100 training steps: 0.007969373282147757\n",
      "Training loss per 100 training steps: 0.007931262365253204\n",
      "Training loss per 100 training steps: 0.007832594936012047\n",
      "Training loss per 100 training steps: 0.007784032206522121\n",
      "Training loss per 100 training steps: 0.007784937178012338\n",
      "Training loss per 100 training steps: 0.007808340891585175\n",
      "Training loss per 100 training steps: 0.007724905964645024\n",
      "Training loss per 100 training steps: 0.007713712462229301\n",
      "Training loss epoch: 0.007803574839172243\n",
      "Training accuracy epoch: 0.9960369204553022\n",
      "Training epoch: 10\n",
      "Training loss per 100 training steps: 0.00527167459949851\n",
      "Training loss per 100 training steps: 0.004888105410638745\n",
      "Training loss per 100 training steps: 0.006393138520881205\n",
      "Training loss per 100 training steps: 0.006471837959200085\n",
      "Training loss per 100 training steps: 0.00673838180253306\n",
      "Training loss per 100 training steps: 0.006996439817963826\n",
      "Training loss per 100 training steps: 0.007029234427228855\n",
      "Training loss per 100 training steps: 0.00694413578293903\n",
      "Training loss per 100 training steps: 0.00679090056201516\n",
      "Training loss per 100 training steps: 0.006875089360451748\n",
      "Training loss per 100 training steps: 0.006746096973081869\n",
      "Training loss per 100 training steps: 0.006666678799716022\n",
      "Training loss per 100 training steps: 0.006815926041556583\n",
      "Training loss per 100 training steps: 0.006826618108402674\n",
      "Training loss per 100 training steps: 0.00685402428832012\n",
      "Training loss epoch: 0.0068763506346590725\n",
      "Training accuracy epoch: 0.9963885371081959\n",
      "Training epoch: 11\n",
      "Training loss per 100 training steps: 0.009633118286728859\n",
      "Training loss per 100 training steps: 0.007656066492027004\n",
      "Training loss per 100 training steps: 0.006444955269130762\n",
      "Training loss per 100 training steps: 0.006478365151714603\n",
      "Training loss per 100 training steps: 0.0068169596376817234\n",
      "Training loss per 100 training steps: 0.006555892392795927\n",
      "Training loss per 100 training steps: 0.0065046148510026465\n",
      "Training loss per 100 training steps: 0.006395668248226135\n",
      "Training loss per 100 training steps: 0.0063552951517461985\n",
      "Training loss per 100 training steps: 0.006266372775274155\n",
      "Training loss per 100 training steps: 0.0062540031476955495\n",
      "Training loss per 100 training steps: 0.006175670003194831\n",
      "Training loss per 100 training steps: 0.006183942306752389\n",
      "Training loss per 100 training steps: 0.006205007926069842\n",
      "Training loss per 100 training steps: 0.00617641110752587\n",
      "Training loss epoch: 0.006195031617060842\n",
      "Training accuracy epoch: 0.996884330886001\n",
      "Training epoch: 12\n",
      "Training loss per 100 training steps: 0.0036032325588166714\n",
      "Training loss per 100 training steps: 0.004056654707989701\n",
      "Training loss per 100 training steps: 0.00468443540225804\n",
      "Training loss per 100 training steps: 0.004845681862315906\n",
      "Training loss per 100 training steps: 0.004907393884844933\n",
      "Training loss per 100 training steps: 0.004953463731089687\n",
      "Training loss per 100 training steps: 0.004949571260522967\n",
      "Training loss per 100 training steps: 0.0050025524787180335\n",
      "Training loss per 100 training steps: 0.004924193390690468\n",
      "Training loss per 100 training steps: 0.005055138148371701\n",
      "Training loss per 100 training steps: 0.005057274990056223\n",
      "Training loss per 100 training steps: 0.0051070880970533455\n",
      "Training loss per 100 training steps: 0.005169333426645571\n",
      "Training loss per 100 training steps: 0.005410265523459832\n",
      "Training loss per 100 training steps: 0.005895452352093002\n",
      "Training loss epoch: 0.005952150999849107\n",
      "Training accuracy epoch: 0.9969682670993285\n",
      "Training epoch: 13\n",
      "Training loss per 100 training steps: 0.0016685082809999585\n",
      "Training loss per 100 training steps: 0.004133202317894072\n",
      "Training loss per 100 training steps: 0.004373383141406991\n",
      "Training loss per 100 training steps: 0.004622053021601919\n",
      "Training loss per 100 training steps: 0.004813161387646506\n",
      "Training loss per 100 training steps: 0.004805557326024595\n",
      "Training loss per 100 training steps: 0.0047728984181640574\n",
      "Training loss per 100 training steps: 0.004678126911490063\n",
      "Training loss per 100 training steps: 0.004529797709645631\n",
      "Training loss per 100 training steps: 0.004573910373530552\n",
      "Training loss per 100 training steps: 0.004660376044086405\n",
      "Training loss per 100 training steps: 0.004563127411790458\n",
      "Training loss per 100 training steps: 0.0045960771585048165\n",
      "Training loss per 100 training steps: 0.004543393008669934\n",
      "Training loss per 100 training steps: 0.004609815449753175\n",
      "Training loss epoch: 0.004566672127004375\n",
      "Training accuracy epoch: 0.9976780286841078\n",
      "Training epoch: 14\n",
      "Training loss per 100 training steps: 0.00023214065004140139\n",
      "Training loss per 100 training steps: 0.003695897702608917\n",
      "Training loss per 100 training steps: 0.0036794419592754354\n",
      "Training loss per 100 training steps: 0.003562612768853742\n",
      "Training loss per 100 training steps: 0.00356097103094567\n",
      "Training loss per 100 training steps: 0.003542526857495386\n",
      "Training loss per 100 training steps: 0.0037531117434199996\n",
      "Training loss per 100 training steps: 0.004001595651505226\n",
      "Training loss per 100 training steps: 0.004308051210683514\n",
      "Training loss per 100 training steps: 0.004726001848412912\n",
      "Training loss per 100 training steps: 0.004785911782591511\n",
      "Training loss per 100 training steps: 0.004960261802800074\n",
      "Training loss per 100 training steps: 0.004894523101427665\n",
      "Training loss per 100 training steps: 0.004872146433467378\n",
      "Training loss per 100 training steps: 0.004837837251881519\n",
      "Training loss epoch: 0.00481796842069883\n",
      "Training accuracy epoch: 0.9976154719346084\n",
      "Training epoch: 15\n",
      "Training loss per 100 training steps: 0.008630878292024136\n",
      "Training loss per 100 training steps: 0.00250608899393165\n",
      "Training loss per 100 training steps: 0.0026360712334244636\n",
      "Training loss per 100 training steps: 0.002962189042240987\n",
      "Training loss per 100 training steps: 0.00340535830157631\n",
      "Training loss per 100 training steps: 0.003441188923442135\n",
      "Training loss per 100 training steps: 0.003508369226082309\n",
      "Training loss per 100 training steps: 0.0035747859284296305\n",
      "Training loss per 100 training steps: 0.0036234741324647182\n",
      "Training loss per 100 training steps: 0.0036738868734932117\n",
      "Training loss per 100 training steps: 0.003734025928082389\n",
      "Training loss per 100 training steps: 0.003958110243723824\n",
      "Training loss per 100 training steps: 0.004017486386890279\n",
      "Training loss per 100 training steps: 0.0039763289543049505\n",
      "Training loss per 100 training steps: 0.003953185182121099\n",
      "Training loss epoch: 0.003971457486957431\n",
      "Training accuracy epoch: 0.9980130131709118\n",
      "Training epoch: 16\n",
      "Training loss per 100 training steps: 0.002422954887151718\n",
      "Training loss per 100 training steps: 0.0029477431165618653\n",
      "Training loss per 100 training steps: 0.0033289024090912456\n",
      "Training loss per 100 training steps: 0.003171445458361251\n",
      "Training loss per 100 training steps: 0.0034069997115768543\n",
      "Training loss per 100 training steps: 0.0033131431490930224\n",
      "Training loss per 100 training steps: 0.003237107569077485\n",
      "Training loss per 100 training steps: 0.0032465559138894263\n",
      "Training loss per 100 training steps: 0.0031965803357113204\n",
      "Training loss per 100 training steps: 0.0032590856908821346\n",
      "Training loss per 100 training steps: 0.003208083268371004\n",
      "Training loss per 100 training steps: 0.0032566403819944195\n",
      "Training loss per 100 training steps: 0.003208515372003137\n",
      "Training loss per 100 training steps: 0.003312968773359082\n",
      "Training loss per 100 training steps: 0.0033550788431173503\n",
      "Training loss epoch: 0.0034118452717461127\n",
      "Training accuracy epoch: 0.9982989178807362\n",
      "Training epoch: 17\n",
      "Training loss per 100 training steps: 0.00337720918469131\n",
      "Training loss per 100 training steps: 0.0034150594096494804\n",
      "Training loss per 100 training steps: 0.0033978631482428473\n",
      "Training loss per 100 training steps: 0.0030816478125277333\n",
      "Training loss per 100 training steps: 0.0029535534346027737\n",
      "Training loss per 100 training steps: 0.0031261493533562687\n",
      "Training loss per 100 training steps: 0.003124053265054319\n",
      "Training loss per 100 training steps: 0.003065590118088126\n",
      "Training loss per 100 training steps: 0.0030969247641081843\n",
      "Training loss per 100 training steps: 0.003146119635480232\n",
      "Training loss per 100 training steps: 0.0031966003205908763\n",
      "Training loss per 100 training steps: 0.0031970236881241794\n",
      "Training loss per 100 training steps: 0.0032021080129488594\n",
      "Training loss per 100 training steps: 0.003295223349350824\n",
      "Training loss per 100 training steps: 0.0036147058498351313\n",
      "Training loss epoch: 0.003657687185019703\n",
      "Training accuracy epoch: 0.9981032612047847\n",
      "Training epoch: 18\n",
      "Training loss per 100 training steps: 0.012281584553420544\n",
      "Training loss per 100 training steps: 0.00449634848058003\n",
      "Training loss per 100 training steps: 0.003529553368048142\n",
      "Training loss per 100 training steps: 0.003271099683220759\n",
      "Training loss per 100 training steps: 0.003276479848678274\n",
      "Training loss per 100 training steps: 0.0032691469147000667\n",
      "Training loss per 100 training steps: 0.003159564969608863\n",
      "Training loss per 100 training steps: 0.003008275230490686\n",
      "Training loss per 100 training steps: 0.003084444711633213\n",
      "Training loss per 100 training steps: 0.0030315685503839356\n",
      "Training loss per 100 training steps: 0.0030336182377880675\n",
      "Training loss per 100 training steps: 0.0030165684543288105\n",
      "Training loss per 100 training steps: 0.003108136209621693\n",
      "Training loss per 100 training steps: 0.0032724234228223327\n",
      "Training loss per 100 training steps: 0.0033210727525692166\n",
      "Training loss epoch: 0.0033351143640030076\n",
      "Training accuracy epoch: 0.9983574706301277\n",
      "Training epoch: 19\n",
      "Training loss per 100 training steps: 0.0014677243307232857\n",
      "Training loss per 100 training steps: 0.0036317607946780343\n",
      "Training loss per 100 training steps: 0.0031378600366649754\n",
      "Training loss per 100 training steps: 0.003127284414896647\n",
      "Training loss per 100 training steps: 0.0031054433170073733\n",
      "Training loss per 100 training steps: 0.003006647735782972\n",
      "Training loss per 100 training steps: 0.00282748983915921\n",
      "Training loss per 100 training steps: 0.002707480753876855\n",
      "Training loss per 100 training steps: 0.002769951326438567\n",
      "Training loss per 100 training steps: 0.0027910453670698353\n",
      "Training loss per 100 training steps: 0.002756145555891139\n",
      "Training loss per 100 training steps: 0.002756551141439747\n",
      "Training loss per 100 training steps: 0.002773711923787001\n",
      "Training loss per 100 training steps: 0.002721071331546966\n",
      "Training loss per 100 training steps: 0.002688878544715153\n",
      "Training loss epoch: 0.0026676504659285407\n",
      "Training accuracy epoch: 0.9986746938220931\n",
      "Training epoch: 20\n",
      "Training loss per 100 training steps: 4.3815798562718555e-05\n",
      "Training loss per 100 training steps: 0.001490515489296182\n",
      "Training loss per 100 training steps: 0.0020651773391554304\n",
      "Training loss per 100 training steps: 0.002947404994891113\n",
      "Training loss per 100 training steps: 0.0027960940106933952\n",
      "Training loss per 100 training steps: 0.002575579885051629\n",
      "Training loss per 100 training steps: 0.0024266860657549544\n",
      "Training loss per 100 training steps: 0.0022249889607545126\n",
      "Training loss per 100 training steps: 0.002225697701579973\n",
      "Training loss per 100 training steps: 0.002263288062186519\n",
      "Training loss per 100 training steps: 0.0022497981579001495\n",
      "Training loss per 100 training steps: 0.0022651776453453043\n",
      "Training loss per 100 training steps: 0.002354525556061325\n",
      "Training loss per 100 training steps: 0.00238254150919041\n",
      "Training loss per 100 training steps: 0.002389867689706159\n",
      "Training loss epoch: 0.0024880121953881344\n",
      "Training accuracy epoch: 0.9987358106348521\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['ids']\n",
    "            mask = batch['mask']\n",
    "            targets = batch['targets']\n",
    "            \n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.037723347544670105\n",
      "Validation loss per 100 evaluation steps: 0.07156972740232401\n",
      "Validation loss per 100 evaluation steps: 0.0596951966621106\n",
      "Validation loss per 100 evaluation steps: 0.06275576063589568\n",
      "Validation loss per 100 evaluation steps: 0.05890676725271407\n",
      "Validation loss per 100 evaluation steps: 0.06032970593179322\n",
      "Validation loss per 100 evaluation steps: 0.05834420383165217\n",
      "Validation loss per 100 evaluation steps: 0.057707627551374305\n",
      "Validation loss per 100 evaluation steps: 0.058281365073936\n",
      "Validation loss per 100 evaluation steps: 0.057181655127420045\n",
      "Validation loss per 100 evaluation steps: 0.05673453858639346\n",
      "Validation loss per 100 evaluation steps: 0.05672770538586122\n",
      "Validation loss per 100 evaluation steps: 0.05667013730295195\n",
      "Validation Loss: 0.05699603785483859\n",
      "Validation Accuracy: 0.9839012716252663\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'TRIVIAL',\n",
       "  'score': 0.7321303,\n",
       "  'word': 'Propane',\n",
       "  'start': 1,\n",
       "  'end': 8},\n",
       " {'entity_group': 'FORMULA',\n",
       "  'score': 0.7086718,\n",
       "  'word': 'C3H8',\n",
       "  'start': 64,\n",
       "  'end': 68},\n",
       " {'entity_group': 'TRIVIAL',\n",
       "  'score': 0.9166991,\n",
       "  'word': 'Propane',\n",
       "  'start': 455,\n",
       "  'end': 462},\n",
       " {'entity_group': 'SYSTEMATIC',\n",
       "  'score': 0.9988894,\n",
       "  'word': 'butane',\n",
       "  'start': 541,\n",
       "  'end': 547},\n",
       " {'entity_group': 'SYSTEMATIC',\n",
       "  'score': 0.9974094,\n",
       "  'word': 'propylene',\n",
       "  'start': 549,\n",
       "  'end': 558},\n",
       " {'entity_group': 'SYSTEMATIC',\n",
       "  'score': 0.99758244,\n",
       "  'word': 'butadiene',\n",
       "  'start': 560,\n",
       "  'end': 569},\n",
       " {'entity_group': 'SYSTEMATIC',\n",
       "  'score': 0.99358565,\n",
       "  'word': 'butylene',\n",
       "  'start': 571,\n",
       "  'end': 579},\n",
       " {'entity_group': 'TRIVIAL',\n",
       "  'score': 0.9250084,\n",
       "  'word': 'isobutylene',\n",
       "  'start': 581,\n",
       "  'end': 592},\n",
       " {'entity_group': 'TRIVIAL',\n",
       "  'score': 0.8556896,\n",
       "  'word': 'Propane',\n",
       "  'start': 616,\n",
       "  'end': 623},\n",
       " {'entity_group': 'TRIVIAL',\n",
       "  'score': 0.86377996,\n",
       "  'word': 'Propane',\n",
       "  'start': 745,\n",
       "  'end': 752},\n",
       " {'entity_group': 'TRIVIAL',\n",
       "  'score': 0.8865411,\n",
       "  'word': 'Propane',\n",
       "  'start': 925,\n",
       "  'end': 932}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"token-classification\", model=model.to(\"cpu\"), tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "txt = \"'Propane () is a three-carbon alkane with the molecular formula C3H8. It is a gas at standard temperature and pressure, but compressible to a transportable liquid. A by-product of natural gas processing and petroleum refining, it is commonly used as a fuel in domestic and industrial applications and in low-emissions public transportation. Discovered in 1857 by the French chemist Marcellin Berthelot, it became commercially available in the US by 1911. Propane is one of a group of liquefied petroleum gases (LP gases). The others include butane, propylene, butadiene, butylene, isobutylene, and mixtures thereof. Propane has lower volumetric energy density, but higher gravimetric energy density and burns more cleanly than gasoline and coal.Propane gas has become a popular choice for barbecues and portable stoves because its low boiling point makes it vaporize as soon as it is released from its pressurized container. Propane powers buses, forklifts, taxis, outboard boat motors, and ice resurfacing machines and is used for heat and cooking in recreational vehicles and campers.'\"\n",
    "pipe(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chemical_tokenizer/tokenizer_config.json',\n",
       " 'chemical_tokenizer/special_tokens_map.json',\n",
       " 'chemical_tokenizer/vocab.txt',\n",
       " 'chemical_tokenizer/added_tokens.json',\n",
       " 'chemical_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"chemical_NER_model\")\n",
    "tokenizer.save_pretrained(\"chemical_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('el-demo': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b200568d40f624367c61b4a8dd9407fba3f4ffafe697cbacc172a365bc7f88e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
